# DeformAR-Extraction

This repository accompanies our EMNLP 2025 System Demonstration submission. It contains the **data extraction pipeline** for DeformAR, focused on converting raw Arabic text into structured outputs (entities, relations). The pipeline is designed for **easy reproduction in Google Colab**, and integrates seamlessly with Google Drive.

---

## ðŸ§ª Running the DeformAR Extraction Pipeline

All notebooks are located in the `reproducability/notebooks/` directory.

### Step 1: Generate Corpora
Convert raw data into structured corpora (e.g., JSON or plain text formats).

Run: [`00-generate-corpora.ipynb`](reproducability/notebooks/00-generate-corpora.ipynb)

### Step 2: Initialize Configuration
Prepare experiment and extraction configurations.

- [`01-initialize-anercorp.ipynb`](reproducability/notebooks/01-initialize-anercorp.ipynb)
- [`01-initialize-conll2003.ipynb`](reproducability/notebooks/01-initialize-conll2003.ipynb)

### Step 3: Fine-tune and Extract Data
Fine-tune models and extract structured output.

- [`02-extract-anercorp.ipynb`](reproducability/notebooks/02-extract-anercorp.ipynb)
- [`02-extract-conll2003.ipynb`](reproducability/notebooks/02-extract-conll2003.ipynb)

---

## âš™ï¸ Quickstart (Google Colab)

The pipeline is designed and tested in **Google Colab with GPU**. Local use is possible but may require path and device configuration changes.

To run in Colab:

```python
# Install the package using your GitHub token (add as a secret in Colab)
if 'google.colab' in str(get_ipython()):
    from google.colab import userdata
    access_token = userdata.get('DEFORMER_TOKEN')  # Add this in Colab > Secrets
    !pip install git+https://$access_token@github.com/ay94/deformer-extraction.git@main
```
## Project Structure

Here's a brief overview of the key components in this library:

- **`analysis.py`**: Contains functions and classes for performing data analysis, including statistical summaries, visualizations, and model performance analysis.

- **`config_managers.py`**: Manages configurations across the library, ensuring consistency and ease of setup for different environments and experiments.

- **`env_setup.py`**: Handles the environment setup, distinguishing between local environments and cloud-based platforms like Google Colab. It ensures that the necessary directories and paths are properly configured.

- **`evaluation.py`**: Implements evaluation strategies for different tasks, such as token and entity level evaluation, including metrics like precision, recall, and F1 score.

- **`model_outputs.py`**: Processes and manages the outputs generated by models, including saving, loading, and preparing outputs for further analysis.

- **`pipelines.py`**: Orchestrates the workflow pipelines for the library, including data preprocessing, training, and evaluation steps. This file ensures that the various components work together smoothly.

- **`tokenization.py`**: Handles the tokenization process, which is essential for NLP tasks. It includes strategies for splitting text into tokens and managing tokenization configurations.

- **`train.py`**: Manages the training process for machine learning models. This file includes the logic for setting up the model, loading data, and running training loops.

- **`utils.py`**: A collection of utility functions and classes used throughout the library. This includes file handling, data loading, and other helper functions.

## Getting Started

### Installation

To use this library, clone the repository and install the required dependencies:

```bash
git clone https://github.com/your-repo/my-library.git
cd my-library
pip install -r requirements.txt
```

## Usage

1. **Setup the Environment**: Before running any scripts, ensure that your environment is properly set up. You can do this by configuring the paths in `env_setup.py` or setting up a Google Colab environment.

2. **Configure Your Experiment**: Use `config_managers.py` to set up and manage your experiment configurations. This will allow you to easily adjust settings such as batch size, learning rate, and other hyperparameters.

3. **Tokenize Your Data**: If you're working with NLP tasks, use `tokenization.py` to preprocess and tokenize your data according to your specified strategy.

4. **Train Your Model**: Use `train.py` to initiate the training process. This script handles loading the data, setting up the model, and running the training loop.

5. **Evaluate Your Model**: Once the model is trained, use `evaluation.py` to assess its performance. This will generate metrics and reports that help you understand how well your model is performing.

6. **Analyze the Results**: Finally, use `analysis.py` to analyze the model outputs. This can include generating visualizations, computing additional statistics, and more.

## Experiment Structure

When running experiments, all data and configurations are organized and saved in a structured way within a local directory. Hereâ€™s how the structure is organized:

- **`experiments/`**: The root folder where all experiments are stored.
  - **`<experiment_name>/`**: Each experiment has its own folder named after the experiment.
    - **`<variant>/`**: Within each experiment folder, there are subfolders for different variants of the experiment. Each variant represents a specific configuration or set of conditions under which the experiment is run.
      - **`configs/`**: Contains configuration files that define the setup for the experiment. This includes settings for the model, data processing, and other parameters.
      - **`extraction/`**: Holds the intermediate outputs generated during the data extraction process. This may include processed datasets, tokenized text, etc.
      - **`fine_tuning/`**: Contains outputs related to the fine-tuning of models. This includes model checkpoints, logs, and any other artifacts generated during the fine-tuning process.

### Example Directory Structure


```
experiments/
â”œâ”€â”€ experiment_1/
â”‚ â”œâ”€â”€ variant_a/
â”‚ â”‚ â”œâ”€â”€ configs/
â”‚ â”‚	â””â”€â”€â”€â”€â”€â”€	â”œâ”€â”€ experiment.yaml
â”‚ â”‚ 	 	â”œâ”€â”€ extraction.yaml
â”‚ â”‚ 	 	â”œâ”€â”€ fine_tuning.yaml
â”‚ â”‚ 		â””â”€â”€ results.yaml
â”‚ â”‚ â”œâ”€â”€ extraction/
â”‚ â”‚	â””â”€â”€â”€â”€â”€â”€â”œâ”€â”€ analysis_data.json
â”‚ â”‚ 	   â”œâ”€â”€ attention_similarity_matrix.json
â”‚ â”‚ 	   â”œâ”€â”€ attention_weights_similarity.json
â”‚ â”‚ 	   â”œâ”€â”€ centroids_avg_similarity_matrix.json
â”‚ â”‚ 	   â”œâ”€â”€ entity_confusion_data.json
â”‚ â”‚ 	   â”œâ”€â”€ kmeans_results.json
â”‚ â”‚ 	   â””â”€â”€ results.json
â”‚ â”‚			
â”‚ â”‚ â”œâ”€â”€ fine_tuning/
â”‚ â”‚	â””â”€â”€â”€â”€â”€â”€â”œâ”€â”€ evaluation_metrics.json
â”‚ â”‚ 	   â”œâ”€â”€ model_binary.bin
â”‚ â”‚ 	   â””â”€â”€ model_state_dict.pth
â”‚ â”œâ”€â”€ variant_b/
â”‚ â”‚ â”œâ”€â”€ configs/
â”‚ â”‚ â”œâ”€â”€ extraction/
â”‚ â”‚ â””â”€â”€ fine_tuning/
â”œâ”€â”€ experiment_2/
â”‚ â”œâ”€â”€ variant_a/
â”‚ â”‚ â”œâ”€â”€ configs/
â”‚ â”‚ â”œâ”€â”€ extraction/
â”‚ â”‚ â””â”€â”€ fine_tuning/
â”‚ â”œâ”€â”€ variant_b/
â”‚ â”‚ â”œâ”€â”€ configs/
â”‚ â”‚ â”œâ”€â”€ extraction/
â”‚ â”‚ â””â”€â”€ fine_tuning/
```


### Explanation

- **Experiments Folder**: All experiments are stored in the `experiments` directory. This is the root directory for your experiment-related files.
- **Experiment Name Folder**: Each experiment has its own folder, named appropriately (e.g., `experiment_1`). This folder contains all the data and outputs related to that specific experiment.
- **Variant Folder**: Inside each experiment's folder, there are subfolders for different variants of the experiment. Variants allow you to run the same experiment under different configurations or conditions, and keep their outputs organized separately.
- **Configs Folder**: Each variant contains a `configs` folder that stores configuration files specific to that variant. This allows you to easily manage and switch between different setups.
- **Extraction Folder**: The `extraction` folder holds outputs from data extraction processes, such as preprocessed datasets.
- **Fine Tuning Folder**: The `fine_tuning` folder contains artifacts from the fine-tuning stage, such as model checkpoints and logs.

