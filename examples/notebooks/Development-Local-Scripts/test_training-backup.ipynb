{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import userdata\n",
    "    access_token = userdata.get('DEFORMER_TOKEN')\n",
    "    !pip install git+https://$access_token@github.com/ay94/deformer-extractor.git@error-handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_utils.configurations import (\n",
    "    TrainingConfig,\n",
    "    TokenizationConfig,\n",
    "    ModelConfig\n",
    ")\n",
    "\n",
    "from experiment_utils.train import (\n",
    "    DatasetManager,\n",
    "    ModelManager,\n",
    "    FineTuneUtils,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 07:48:06 - INFO - PyTorch version 1.13.1 available.\n"
     ]
    }
   ],
   "source": [
    "from experiment_utils import colab\n",
    "from experiment_utils.general_utils import FileHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 07:48:06 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/notebooks/My Drive\n",
      "2024-07-20 07:48:06 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n"
     ]
    }
   ],
   "source": [
    "from experiment_utils import colab\n",
    "from experiment_utils.general_utils import FileHandler\n",
    "\n",
    "base_folder = colab.init('My Drive')\n",
    "corpora_path = base_folder / 'Final Year Experiments/Thesis-Experiments/ExperimentData'\n",
    "corpora_fh = FileHandler(corpora_path)\n",
    "corpora  = corpora_fh.load_json('corpora.json')\n",
    "\n",
    "config = base_folder / 'Final Year Experiments/Thesis-Experiments/scripts'\n",
    "config_fh = FileHandler(config)\n",
    "config = config_fh.load_yaml('config.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 07:48:06 - INFO - Configuration validated successfully\n"
     ]
    }
   ],
   "source": [
    "finetune_config = TrainingConfig.from_dict(config.get('finetuning'))\n",
    "tokenization_config = TokenizationConfig.from_dict(config.get('tokenization'))\n",
    "model_config = ModelConfig.from_dict(config.get('model'))\n",
    "data_manager = DatasetManager(corpora, 'ANERCorp_CamelLab', tokenization_config)\n",
    "num_tags = len(data_manager.corpus['labels'])\n",
    "model_manager = ModelManager(num_tags, model_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 07:48:06 - INFO - Loading BERT Model from: aubmindlab/bert-base-arabertv02\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-07-20 07:48:10 - INFO - Model aubmindlab/bert-base-arabertv02 loaded and sent to cpu\n",
      "2024-07-20 07:48:10 - INFO - Loading Preprocessor: aubmindlab/bert-base-arabertv02\n",
      "2024-07-20 07:48:10 - INFO - Loading Tokenizer: aubmindlab/bert-base-arabertv02, lower_case: False\n",
      "2024-07-20 07:48:11 - INFO - Loading Preprocessor: aubmindlab/bert-base-arabertv02\n",
      "2024-07-20 07:48:11 - INFO - Loading Tokenizer: aubmindlab/bert-base-arabertv02, lower_case: False\n",
      "2024-07-20 07:48:11 - ERROR - The validation Split Doesn't Exist\n",
      "2024-07-20 07:48:11 - INFO - num_train_steps: 1037\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(data_manager, model_manager, finetune_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 07:48:12 - INFO - Standard\n",
      "2024-07-20 07:48:12 - INFO - Start Training Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a288216b5cbb442bb07e8dddcaab0079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/train.py:419\u001b[0m, in \u001b[0;36mTrainer.training_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandard_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/train.py:444\u001b[0m, in \u001b[0;36mTrainer.standard_training_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Training Epoch: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    443\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 444\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_dataloader:\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/train.py:421\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 421\u001b[0m     training_loss \u001b[38;5;241m=\u001b[39m \u001b[43mFineTuneUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_loss\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/train.py:307\u001b[0m, in \u001b[0;36mFineTuneUtils.train_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler, args)\u001b[0m\n\u001b[1;32m    305\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    306\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(data_loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    308\u001b[0m     data \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = trainer.training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 16:45:48 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/notebooks/My Drive\n",
      "2024-07-19 16:45:48 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_folder = colab.init('My Drive')\n",
    "config_path = base_folder / 'Final Year Experiments/Thesis-Experiments/scripts'\n",
    "data_path = base_folder / 'Final Year Experiments/Thesis-Experiments/ExperimentData'\n",
    "fh = FileHandler(config_path)\n",
    "data_fh = FileHandler(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FineTuneConfig' from 'experiment_utils.train' (/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/train.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiment_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FineTuneConfig, TCDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiment_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenStrategyFactory\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FineTuneConfig' from 'experiment_utils.train' (/Users/ay227/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/train.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from experiment_utils.train import FineTuneConfig, TCDataset\n",
    "from experiment_utils.tokenization import TokenStrategyFactory\n",
    "\n",
    "\n",
    "# Example usage\n",
    "config_values = fh.load_yaml('train_config.yaml')\n",
    "tokenization_config = fh.load_yaml('tokenization_config.yaml')\n",
    "config = FineTuneConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = data_fh.load_json('corpora.json')\n",
    "NERData = data['ANERCorp_CamelLab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]['input_ids'][:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]['labels'][:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TCModel(num_tag=10, path='bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, BertConfig\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a custom dataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, tags, label_map):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\",\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "        labels = [self.label_map[tag] for tag in self.tags[idx]]\n",
    "        labels = torch.tensor(labels + [self.label_map['O']] * (128 - len(labels)))  # Pad labels\n",
    "        return tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), labels\n",
    "\n",
    "# Dummy data\n",
    "texts = [\n",
    "    [\"Hello\", \"world\", \"this\", \"is\", \"BERT\"],\n",
    "    [\"Another\", \"sentence\"]\n",
    "]\n",
    "tags = [\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-PER\"],\n",
    "    [\"O\", \"O\"]\n",
    "]\n",
    "label_map = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2}\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = NERDataset(tokenizer, texts, tags, label_map)\n",
    "loader = DataLoader(dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model output without hidden states and attentions\n",
    "for batch in loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_hidden_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model output without hidden states and attentions\n",
    "for batch in loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    token_type_ids = torch.tensor([0] * input_ids.shape[1], dtype=torch.long)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = TCModel(num_tag=10, path='bert-base-uncased')\n",
    "\n",
    "loaded_model.load_state_dict(torch.load('model_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model output without hidden states and attentions\n",
    "for batch in loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    token_type_ids = torch.tensor([0] * input_ids.shape[1], dtype=torch.long)\n",
    "    outputs = loaded_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.enable_attentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model output without hidden states and attentions\n",
    "for batch in loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    token_type_ids = torch.tensor([0] * input_ids.shape[1], dtype=torch.long)\n",
    "    outputs = loaded_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 16:45:54 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/notebooks/My Drive\n",
      "2024-07-19 16:45:54 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n"
     ]
    }
   ],
   "source": [
    "base_folder = colab.init('My Drive')\n",
    "config_path = base_folder / 'Final Year Experiments/Class Imbalance/1_fineTuning'\n",
    "fh = FileHandler(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='arabertv02'\n",
    "data_name='ANERCorp_CamelLab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "model_outputs = fh.load_pickle(\n",
    "            f\"evalOutputs/{model_name}_{data_name}_regular_outputs.pkl\"\n",
    "        )\n",
    "\n",
    "# load_model_path = fh.file_path / f\"trainOutputs/{model_name}_{data_name}_regular.bin\"\n",
    "# model = torch.load(load_model_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_loader = model_outputs.val_dataloader\n",
    "\n",
    "# Assuming 'data_loader' is your DataLoader instance\n",
    "# and that it has an attribute 'dataset' which stores the tensors\n",
    "\n",
    "data = [(data_loader.dataset[i]) for i in range(len(data_loader.dataset))]\n",
    "\n",
    "# Save to disk\n",
    "torch.save(data, 'val.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2, 40508, 32479,    19, 33014, 34675, 43752,   466,    19,  2279,\n",
       "         25896,  1089,  1098,  1161,   921,  7410, 16661,   306,  2767,  4119,\n",
       "         19020, 31490,  8500, 10926, 18694,  1000, 13567,   139,  2801,  3011,\n",
       "          7996,   305,  7377, 40508, 50014,   197,   305, 26258,  2050,   492,\n",
       "          3812,  6700, 18112,   306,  9520,  2657, 20195,    20,     3,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([-100,    5,    5,    0,    1,    2, -100, -100,    0,    0,    0,    0,\n",
       "            1,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, -100,    0,    0,    0,    0,    0,    0,    0,    5,    5, -100,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]),\n",
       " 'words_ids': tensor([-100,    0,    1,    2,    3,    4,    4,    4,    5,    6,    7,    8,\n",
       "            9,   10,   11,   12,   13,   14,   15,   16,   17,   18,   19,   20,\n",
       "           21,   21,   22,   23,   24,   25,   26,   27,   28,   29,   30,   30,\n",
       "           31,   32,   33,   34,   35,   36,   37,   38,   39,   40,   41,   42,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]),\n",
       " 'sentence_num': tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "loaded_data = torch.load('test.pth')\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "# Create a new DataLoader\n",
    "loaded_data = torch.load('test.pth')\n",
    "new_data_loader = DataLoader(loaded_data, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = model_outputs.test_dataloader\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\tfinal_loss = 0\n",
    "\tpreds = None\n",
    "\tlabels = None\n",
    "\tfor data in tqdm(data_loader, total=len(data_loader)):\n",
    "\t\tfor k, v in data.items():\n",
    "\t\t\tdata[k] = v.to(device)\n",
    "\t\toutputs = model(**data)\n",
    "\t\tloss = outputs[\"average_loss\"]\n",
    "\t\tlogits = outputs[\"logits\"]\n",
    "\t\tfinal_loss += loss.item()\n",
    "\t\tif logits is not None:\n",
    "\t\t\tpreds = (\n",
    "\t\t\t\tlogits if preds is None else torch.cat((preds, logits), dim=0)\n",
    "\t\t\t)\n",
    "\t\tif data[\"labels\"] is not None:\n",
    "\t\t\tlabels = (\n",
    "\t\t\t\tdata[\"labels\"]\n",
    "\t\t\t\tif labels is None\n",
    "\t\t\t\telse torch.cat((labels, data[\"labels\"]), dim=0)\n",
    "\t\t\t)\n",
    "\tpreds = preds.detach().cpu().numpy()\n",
    "\tlabels = labels.cpu().numpy()\n",
    "\t# evaluation = Evaluation(labels, preds, inv_labels, final_loss)\n",
    "\t# metrics = evaluation.compute_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_utils.train import Evaluation, Metrics\n",
    "\n",
    "evaluator = Evaluation(model_outputs.data['inv_labels'], labels, preds, final_loss)\n",
    "results = evaluator.generate_results()\n",
    "metrics = Metrics.from_dict(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.test_metrics.skl_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Any, Dict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FineTuneConfig:\n",
    "    train_batch_size: int\n",
    "    test_batch_size: int\n",
    "    shuffle: bool\n",
    "    epochs: int\n",
    "    splits: int\n",
    "    learning_rate: float\n",
    "    warmup_ratio: float\n",
    "    max_grad_norm: float\n",
    "    accumulation_steps: int\n",
    "    logging_step: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.validate_config()\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_dict(config_dict: Dict[str, Any]):\n",
    "        try:\n",
    "            config_dict['learning_rate'] = float(config_dict['learning_rate'])\n",
    "        except ValueError as e:\n",
    "            logging.error(\"Invalid format for learning_rate, needs to be convertible to float\")\n",
    "            raise ValueError(\"Invalid format for learning_rate\") from e\n",
    "        return FineTuneConfig(**config_dict)\n",
    "    \n",
    "    def validate_config(self):\n",
    "        if not (0 < self.learning_rate < 1):\n",
    "            logging.error(\"Invalid learning rate: %s\", self.learning_rate)\n",
    "            raise ValueError(\"Learning rate must be between 0 and 1\")\n",
    "        if not (0 < self.warmup_ratio < 1):\n",
    "            logging.error(\"Invalid warmup ratio: %s\", self.warmup_ratio)\n",
    "            raise ValueError(\"Warmup ratio must be between 0 and 1\")\n",
    "        if self.epochs <= 0:\n",
    "            logging.error(\"Invalid number of epochs: %s\", self.epochs)\n",
    "            raise ValueError(\"Epochs must be greater than 0\")\n",
    "        if self.train_batch_size <= 0 or self.test_batch_size <= 0:\n",
    "            logging.error(\"Invalid batch sizes: Train %s, Valid %s\", self.train_batch_size, self.test_batch_size)\n",
    "            raise ValueError(\"Batch sizes must be greater than 0\")\n",
    "        if self.accumulation_steps < 1:\n",
    "            logging.error(\"Invalid accumulation steps: %s\", self.accumulation_steps)\n",
    "            raise ValueError(\"Accumulation steps must be at least 1\")\n",
    "        if self.logging_step < 1:\n",
    "            logging.error(\"Invalid logging steps: %s\", self.logging_step)\n",
    "            raise ValueError(\"Accumulation steps must be at least 1\")\n",
    "        logging.info(\"Configuration validated successfully\")\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenizationStrategy:\n",
    "    type: str\n",
    "    index: int\n",
    "    schema: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class TokenizationConfig:\n",
    "    tokenizer_path: str\n",
    "    preprocessor_path: str\n",
    "    max_seq_len: int\n",
    "    strategy: TokenizationStrategy\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.validate_config()\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_dict(config_dict: Dict[str, Any]):\n",
    "        # This assumes the 'strategy' sub-dictionary is properly formatted\n",
    "        strategy_config = TokenizationStrategy(**config_dict['strategy'])\n",
    "        return TokenizationConfig(\n",
    "            tokenizer_path=config_dict['tokenizer_path'],\n",
    "            preprocessor_path=config_dict['preprocessor_path'],\n",
    "            max_seq_len=config_dict['max_seq_len'],\n",
    "            strategy=strategy_config\n",
    "        )\n",
    "\n",
    "    def validate_config(self):\n",
    "        if not isinstance(self.max_seq_len, int) or self.max_seq_len <= 0:\n",
    "            raise ValueError(\"max_seq_len must be a positive integer\")\n",
    "        if not self.tokenizer_path:\n",
    "            raise ValueError(\"tokenizer_path cannot be empty\")\n",
    "        if not self.preprocessor_path:\n",
    "            raise ValueError(\"preprocessor_path cannot be empty\")\n",
    "        if not isinstance(self.strategy, TokenizationStrategy):\n",
    "            raise ValueError(\"strategy must be an instance of TokenizationStrategy\")\n",
    "        if self.strategy.type not in ['core', 'all']:  # Example check\n",
    "            raise ValueError(\"Invalid strategy type specified\")\n",
    "        if not isinstance(self.strategy.index, int) or self.strategy.index < 0:\n",
    "            raise ValueError(\"Strategy index must be a non-negative integer\")\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_path: str\n",
    "    dropout_rate: float\n",
    "    enable_attentions: bool\n",
    "    enable_hidden_states: bool\n",
    "    initialize_output_layer: bool\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.validate_config()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(config_dict):\n",
    "        return ModelConfig(**config_dict)\n",
    "\n",
    "    def validate_config(self):\n",
    "        if not self.model_path:\n",
    "            raise ValueError(\"Model path cannot be empty\")\n",
    "        if not (0 <= self.dropout_rate <= 1):\n",
    "            raise ValueError(\"Dropout rate must be between 0 and 1\")\n",
    "        if not isinstance(self.enable_attentions, bool):\n",
    "            raise ValueError(\"enable_attentions must be a boolean value\")\n",
    "        if not isinstance(self.enable_hidden_states, bool):\n",
    "            raise ValueError(\"enable_hidden_states must be a boolean value\")\n",
    "        if not isinstance(self.initialize_output_layer, bool):\n",
    "            raise ValueError(\"initialize_output_layer must be a boolean value\")\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    train_batch_size: int\n",
    "    test_batch_size: int\n",
    "    max_grad_norm: int\n",
    "    accumulation_steps: int\n",
    "    epochs: int\n",
    "    splits: int\n",
    "    shuffle: bool\n",
    "    # Including splits here depends on whether you treat 'splits' as a training process configuration\n",
    "    # or part of a dataset configuration. If 'splits' refers to cross-validation or similar, it might belong here.\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        # Assuming 'config' is a dictionary containing these keys\n",
    "        return TrainingArguments(\n",
    "            train_batch_size=config.train_batch_size,\n",
    "            test_batch_size=config.test_batch_size,\n",
    "            max_grad_norm=config.max_grad_norm,\n",
    "            accumulation_steps=config.accumulation_steps,\n",
    "            epochs=config.epochs,\n",
    "            splits=config.epochs,\n",
    "            shuffle=config.shuffle\n",
    "        )\n",
    "\n",
    "    def validate_arguments(self):\n",
    "        if self.train_batch_size <= 0 or self.test_batch_size <= 0:\n",
    "            raise ValueError(\"Batch sizes must be greater than 0\")\n",
    "        if self.epochs <= 0:\n",
    "            raise ValueError(\"Number of epochs must be positive\")\n",
    "        if self.epochs <= 0:\n",
    "            logging.info(\"Shuffle Argument is %s\", self.shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self, corpora, dataset_name, config, config_dict):\n",
    "        self.corpus = self.get_corpus(dataset_name, corpora)\n",
    "        self.config = config\n",
    "        self.config_dict = config_dict\n",
    "    \n",
    "    def get_corpus(self, data_name, corpora):\n",
    "        if data_name not in corpora:\n",
    "            raise ValueError(f\"Data name {data_name} not found in corpora.\")\n",
    "        return corpora[data_name]\n",
    "    \n",
    "    def get_dataset(self, split):\n",
    "        self.data = self.corpus['splits']\n",
    "        return self.create_dataset(split)\n",
    "\n",
    "    def create_dataset(self, split):\n",
    "        return TCDataset(\n",
    "            texts=[x['words'] for x in self.data[split]],\n",
    "            tags=[x['tags'] for x in self.data[split]],\n",
    "            label_map=self.corpus[\"labels_map\"],\n",
    "            config=self.config,\n",
    "            config_dict=self.config_dict\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def get_dataloader(self, split, batch_size, shuffle=False):\n",
    "        try:\n",
    "            return torch.utils.data.DataLoader(\n",
    "                dataset=self.create_dataset(split),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                num_workers=2\n",
    "            )\n",
    "        except:\n",
    "            logging.error(\"The %s Split Doesn't Exist\", split)\n",
    "        return None\n",
    "    \n",
    "        \n",
    "\n",
    "import logging\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from experiment_utils.train import TCModel, TCDataset, FineTuneUtils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, num_tags, config):\n",
    "        self.num_tags = num_tags\n",
    "        self.config = config\n",
    "                \n",
    "    def configure_model(self):\n",
    "        device = self.get_device()\n",
    "        model = TCModel(self.num_tags, self.config)\n",
    "        model.to(device)\n",
    "        self.original_state = model.state_dict()\n",
    "        logging.info(\"Model %s loaded and sent to %s\", self.config.model_path, device)\n",
    "        return model\n",
    "    \n",
    "    def get_device(self):\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def get_model_parameters(self):\n",
    "        model = self.configure_model()\n",
    "        return model.named_parameters()\n",
    "    \n",
    "    def reset_model(self):\n",
    "        self.model.load_state_dict(self.original_state)  # Reset to initial state\n",
    "        return model.to(self.device) \n",
    "\n",
    "    # def save_model(self, save_path):\n",
    "    #     torch.save(self.model.state_dict(), save_path)\n",
    "    #     logging.info(\"Model saved to %s\", save_path)\n",
    "\n",
    "    # def load_model(self, model_path):\n",
    "    #     self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "    #     logging.info(\"Checkpoint loaded from %s\", model_path)\n",
    "\n",
    "class OptimizerSchedulerManager:\n",
    "    def __init__(self, model_parameters, num_train_examples, config):\n",
    "        self.model_parameters = model_parameters\n",
    "        self.num_train_examples = num_train_examples\n",
    "        self.config = config\n",
    "        \n",
    "    def configure_optimizer_and_scheduler(self):\n",
    "        logging.info(\"Configuring the optimizer with learning rate: %s\", self.config.learning_rate)\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model_parameters if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.model_parameters if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.config.learning_rate)\n",
    "        scheduler = self.configure_scheduler(optimizer)\n",
    "        logging.info(\"Optimizer configured successfully\")\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def configure_scheduler(self, optimizer):\n",
    "        steps_per_epoch = (self.num_train_examples// (self.config.train_batch_size * self.config.accumulation_steps))\n",
    "        total_steps = steps_per_epoch * self.config.epochs\n",
    "        logging.info(\"Configuring scheduler with total steps: %d\", total_steps)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),  # 10% of training steps for warmup\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        logging.info(\"Scheduler configured successfully\")\n",
    "        return scheduler\n",
    "    \n",
    "    def reset_optimizer_and_scheduler(self):\n",
    "        return self.configure_optimizer_and_scheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 00:00:45 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/notebooks/My Drive\n",
      "2024-07-19 00:00:45 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n"
     ]
    }
   ],
   "source": [
    "from experiment_utils import colab\n",
    "from experiment_utils.general_utils import FileHandler\n",
    "\n",
    "base_folder = colab.init('My Drive')\n",
    "corpora_path = base_folder / 'Final Year Experiments/Thesis-Experiments/ExperimentData'\n",
    "corpora_fh = FileHandler(corpora_path)\n",
    "corpora  = corpora_fh.load_json('corpora.json')\n",
    "\n",
    "config = base_folder / 'Final Year Experiments/Thesis-Experiments/scripts'\n",
    "config_fh = FileHandler(config)\n",
    "config = config_fh.load_yaml('config.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 00:00:46 - INFO - Configuration validated successfully\n"
     ]
    }
   ],
   "source": [
    "finetune_config = FineTuneConfig.from_dict(config.get('finetuning'))\n",
    "tokenization_config = TokenizationConfig.from_dict(config.get('tokenization'))\n",
    "model_config = ModelConfig.from_dict(config.get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 00:00:49 - INFO - Loading BERT Model from: aubmindlab/bert-base-arabertv02\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-07-19 00:00:53 - INFO - Model aubmindlab/bert-base-arabertv02 loaded and sent to cpu\n",
      "2024-07-19 00:00:53 - INFO - Loading Preprocessor: aubmindlab/bert-base-arabertv02\n",
      "2024-07-19 00:00:53 - INFO - Loading Tokenizer: aubmindlab/bert-base-arabertv02\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change the way the strategy factory works as currently it is restiricted to joson\n",
    "data_manager = DatasetManager(corpora, 'ANERCorp_CamelLab', tokenization_config, config.get('tokenization'))\n",
    "num_tags = len(data_manager.corpus['labels'])\n",
    "model_manager = ModelManager(num_tags, model_config)\n",
    "optimizer_scheduler_manager = OptimizerSchedulerManager(model_manager.get_model_parameters(), len(data_manager.get_dataset('train')), finetune_config)\n",
    "args = TrainingArguments.from_config(finetune_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, finetune_utils, data_manager, model_manager, optimizer_scheduler_manager, args) -> None:\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.train_dataloader = None\n",
    "        self.test_dataloader = None\n",
    "        self.validation_dataloader = None\n",
    "        self.finetune_utils = finetune_utils\n",
    "        self.args = args\n",
    "        self.setup_trainer(data_manager, model_manager, optimizer_scheduler_manager)\n",
    "        \n",
    "    def setup_trainer(self, data_manager, model_manager, optimizer_scheduler_manager):\n",
    "        self.model = model_manager.configure_model()\n",
    "        self.device = model_manager.get_device()\n",
    "        self.optimizer, self.scheduler = optimizer_scheduler_manager.configure_optimizer_and_scheduler()\n",
    "        # self.train_dataloader = data_manager.get_dataloader('train', self.args.train_batch_size)\n",
    "        self.train_dataloader = model_outputs.train_dataloader\n",
    "        # self.test_dataloader = data_manager.get_dataloader('test', self.args.test_batch_size)\n",
    "        self.test_dataloader = model_outputs.test_dataloader\n",
    "        # self.validation_dataloader = data_manager.get_dataloader('validation', self.args.test_batch_size)\n",
    "        \n",
    "    def train(self):\n",
    "        training_loss = self.fine_tune.train_fn(\n",
    "            self.train_dataloader,\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            self.device,\n",
    "            self.scheduler,\n",
    "            self.args,\n",
    "        )\n",
    "        return training_loss\n",
    "    def evaluate(self, dataloader):\n",
    "        eval_metrics, eval_loss = self.fine_tune.eval_fn(\n",
    "            dataloader, self.model, self.device, self.data_manager.corpus.get(\"inv_map\")\n",
    "        )\n",
    "        return eval_metrics, eval_loss\n",
    "    def training_loop(self):\n",
    "        for epoch in range(self.args.epochs):\n",
    "            logging.info(\"Start Training Epoch: %d\", epoch + 1)\n",
    "            start_time = time.time()\n",
    "            training_loss = self.train()\n",
    "\n",
    "            logging.info(\"Start Evaluation\")\n",
    "            eval_metrics, eval_loss = self.evaluate(self.validation_dataloader if self.validation_dataloader else self.test_dataloader)  \n",
    "\n",
    "            logging.info(\"Training Loss: %f | Eval Loss: %f\", training_loss, eval_loss)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time < 3600:\n",
    "                logging.info(\"Epoch completed in %.2f minutes\", elapsed_time / 60)\n",
    "            else:\n",
    "                logging.info(\"Epoch completed in %.2f hours\", elapsed_time / 3600)\n",
    "            print(eval_metrics)\n",
    "    def cross_validation(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x13fa9c190>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs.train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 00:17:28 - INFO - Loading BERT Model from: aubmindlab/bert-base-arabertv02\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-07-19 00:17:33 - INFO - Model aubmindlab/bert-base-arabertv02 loaded and sent to cpu\n",
      "2024-07-19 00:17:33 - INFO - Configuring the optimizer with learning rate: 5e-05\n",
      "2024-07-19 00:17:33 - INFO - Configuring scheduler with total steps: 1036\n",
      "2024-07-19 00:17:33 - INFO - Scheduler configured successfully\n",
      "2024-07-19 00:17:33 - INFO - Optimizer configured successfully\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(FineTuneUtils, data_manager, model_manager, optimizer_scheduler_manager, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(train_batch_size=16, test_batch_size=8, max_grad_norm=1.0, accumulation_steps=1, epochs=4, splits=4, shuffle=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
