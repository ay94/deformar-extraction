{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import userdata\n",
    "    access_token = userdata.get('DEFORMER_TOKEN')\n",
    "    !pip install git+https://$access_token@github.com/ay94/deformer-extractor.git@error-handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Outputs for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:33:19 - INFO - PyTorch version 1.13.1 available.\n",
      "2024-07-28 21:33:21 - WARNING - Resolved path does not exist, checking alternative paths: /Users/ay227/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/notebooks/My Drive\n",
      "2024-07-28 21:33:21 - INFO - Found Google Drive directory for account ahmed.younes.sam@gmail.com: /Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com\n"
     ]
    }
   ],
   "source": [
    "from experiment_utils import colab\n",
    "from experiment_utils.general_utils import FileHandler\n",
    "base_folder = colab.init('My Drive')\n",
    "config_path = base_folder / 'Final Year Experiments/Class Imbalance/1_fineTuning'\n",
    "fh = FileHandler(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_name='arabertv02'\n",
    "data_name='ANERCorp_CamelLab'\n",
    "training_outputs = fh.load_pickle(\n",
    "            f\"evalOutputs/{model_name}_{data_name}_regular_outputs.pkl\"\n",
    "        )\n",
    "\n",
    "load_model_path = fh.file_path / f\"trainOutputs/{model_name}_{data_name}_regular.bin\"\n",
    "model = torch.load(load_model_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_utils.model_outputs import ModelOutputWorkflowManager\n",
    "from experiment_utils.tokenization import TokenizationWorkflowManager\n",
    "from experiment_utils.evaluation import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "            \"token_results\": training_outputs.test_metrics.skl_results,\n",
    "            \"token_report\": training_outputs.test_metrics.skl_report,\n",
    "            \"token_outputs\": training_outputs.test_metrics.skl_output,\n",
    "            \"entity_results\": training_outputs.test_metrics.seq_results,\n",
    "            \"entity_report\": training_outputs.test_metrics.seq_report,\n",
    "            \"entity_outputs\": training_outputs.test_metrics.seq_output\n",
    "        }\n",
    "\n",
    "results = Metrics.from_dict(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_path = base_folder / 'Final Year Experiments/Thesis-Experiments/ExperimentData'\n",
    "corpora_fh = FileHandler(corpora_path)\n",
    "corpora  = corpora_fh.load_json('corpora.json')\n",
    "\n",
    "config = base_folder / 'Final Year Experiments/Thesis-Experiments/scripts'\n",
    "config_fh = FileHandler(config)\n",
    "config = config_fh.load_yaml('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:33:24 - INFO - Configuration validated successfully\n"
     ]
    }
   ],
   "source": [
    "from experiment_utils.train import DatasetManager\n",
    "from experiment_utils.configurations import TrainingConfig, TokenizationConfig\n",
    "split = 'test'\n",
    "args = TrainingConfig.from_dict(config.get('training'))\n",
    "tokenization_config = TokenizationConfig.from_dict(config.get('tokenization'))\n",
    "data_manager = DatasetManager(corpora, 'ANERCorp_CamelLab', tokenization_config)\n",
    "batch_sizes = {'train': args.train_batch_size, 'test': args.test_batch_size, 'validation': args.test_batch_size}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:33:24 - INFO - Loading Tokenizer aubmindlab/bert-base-arabertv02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:33:24 - INFO - Loading Preprocessor aubmindlab/bert-base-arabertv02\n",
      "2024-07-28 21:33:24 - INFO - Processing train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed19da88abac4e80b2e29b55d3ee3afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m NERData \u001b[38;5;241m=\u001b[39m corpora[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANERCorp_CamelLab\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m tokenization_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizationWorkflowManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNERData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenization_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/tokenization.py:452\u001b[0m, in \u001b[0;36mTokenizationWorkflowManager.__init__\u001b[0;34m(self, data, config)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_tokenized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/tokenization.py:482\u001b[0m, in \u001b[0;36mTokenizationWorkflowManager.generate_tokenized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process and store data for all configured splits.\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Manager is not properly configured due to setup failure.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/tokenization.py:406\u001b[0m, in \u001b[0;36mDataSplitManager.process_splits\u001b[0;34m(self, specific_split)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits_to_process:\n\u001b[1;32m    405\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m split\u001b[39m\u001b[38;5;124m\"\u001b[39m, split)\n\u001b[0;32m--> 406\u001b[0m     tokenized_sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    407\u001b[0m         tokenized_sentence_dataset\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tokenized_sentence_dataset \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    409\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_split_processor(split)\n\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m     ]\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    413\u001b[0m         subword_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_split_subwords(split, tokenized_sentences)\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/tokenization.py:406\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits_to_process:\n\u001b[1;32m    405\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m split\u001b[39m\u001b[38;5;124m\"\u001b[39m, split)\n\u001b[0;32m--> 406\u001b[0m     tokenized_sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    407\u001b[0m         tokenized_sentence_dataset\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tokenized_sentence_dataset \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    409\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_split_processor(split)\n\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m     ]\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    413\u001b[0m         subword_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_split_subwords(split, tokenized_sentences)\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/tokenization.py:275\u001b[0m, in \u001b[0;36mTokenizedTextProcessor.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    273\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[index]\n\u001b[1;32m    274\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags[index]\n\u001b[0;32m--> 275\u001b[0m tokens_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m tokens_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_and_add_special_tokens(tokens_data)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TokenizedOutput\u001b[38;5;241m.\u001b[39mfrom_dict(tokens_data)\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/tokenization.py:295\u001b[0m, in \u001b[0;36mTokenizedTextProcessor._tokenize_and_prepare\u001b[0;34m(self, words, tags, index)\u001b[0m\n\u001b[1;32m    281\u001b[0m tokens_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_df\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m    293\u001b[0m }\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_id, (word, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(words, tags)):\n\u001b[0;32m--> 295\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_and_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokens:\n\u001b[1;32m    297\u001b[0m         tokens_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_tokens_data(\n\u001b[1;32m    298\u001b[0m             tokens_data, tokens, index, word, word_id, label\n\u001b[1;32m    299\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/deformer-extractor/experiment_utils/tokenization.py:306\u001b[0m, in \u001b[0;36mTokenizedTextProcessor._preprocess_and_tokenize\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor:\n\u001b[1;32m    305\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mpreprocess(word)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:320\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.tokenize\u001b[0;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, pair: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, add_special_tokens: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokens()\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2702\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2693\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2694\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2695\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2700\u001b[0m )\n\u001b[0;32m-> 2702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    499\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    501\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 502\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/Desktop/Final-Year/Thesis-Experiments/Data-Extraction-Phase/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    422\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    423\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    441\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    443\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    453\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NERData = corpora['ANERCorp_CamelLab']\n",
    "tokenization_outputs = TokenizationWorkflowManager(NERData, tokenization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 23:24:11 - INFO - Specific Split test being processed\n",
      "2024-07-27 23:24:11 - INFO - Loading Preprocessor: aubmindlab/bert-base-arabertv02\n",
      "2024-07-27 23:24:11 - INFO - Loading Tokenizer: aubmindlab/bert-base-arabertv02, lower_case: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e8abc15a924204abe6d11198c16fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model_outputs = ModelOutputWorkflowManager(model, device, data_manager, batch_sizes, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LabelAligner:\n",
    "    def __init__(self, predictions, tokenized_sentences):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreparation class.\n",
    "\n",
    "        Args:\n",
    "            batches: The batches object containing batch data.\n",
    "            tokenization_outputs: The tokenization outputs object.\n",
    "        \"\"\"\n",
    "        self.tokenized_sentences = tokenized_sentences\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def get_alignment_locations(self):\n",
    "        \"\"\"\n",
    "        Create a map for label alignment based on tokenization outputs.\n",
    "\n",
    "        Returns:\n",
    "            alignment_map: A dictionary mapping sentence IDs to token indices and tokens.\n",
    "        \"\"\"\n",
    "        alignment_locations = defaultdict(list)\n",
    "        for sentence_id, sentence in enumerate(self.tokenized_sentences):\n",
    "            for token_label_id, token_label in enumerate(sentence.labels_df):\n",
    "                if token_label in [\"[CLS]\", \"[SEP]\", \"IGNORED\"]:\n",
    "                    alignment_locations[sentence_id].append((token_label_id, token_label))\n",
    "        return alignment_locations\n",
    "\n",
    "    def align_labels(self):\n",
    "        \"\"\"\n",
    "        Modify predictions based on the alignment map.\n",
    "\n",
    "        Args:\n",
    "            preds: A list of predictions.\n",
    "            pred_map: A dictionary mapping sentence IDs to token indices and tokens.\n",
    "\n",
    "        Returns:\n",
    "            modified_preds: A list of modified predictions.\n",
    "        \"\"\"\n",
    "        alignment_locations = self.get_alignment_locations()\n",
    "        modified_predictions = []\n",
    "        for sentence_id, original_sentence in enumerate(self.predictions):\n",
    "            sentence = original_sentence[:]  # Create a shallow copy of the list\n",
    "            for index, token in alignment_locations[sentence_id]:\n",
    "                # no need to shift as the indices were calculated according to the tokenized version in the truth labels\n",
    "                sentence.insert(index, token)\n",
    "            modified_predictions.append(sentence)\n",
    "        return modified_predictions\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnalysisData:\n",
    "    model_outputs: list\n",
    "    tokenization_outputs: list\n",
    "    last_hidden_states: torch.Tensor = field(init=False)\n",
    "    labels: torch.Tensor = field(init=False)\n",
    "    losses: torch.Tensor = field(init=False)\n",
    "    token_ids: torch.Tensor = field(init=False)\n",
    "    words: list = field(init=False)\n",
    "    tokens: list = field(init=False)\n",
    "    word_pieces: list = field(init=False)\n",
    "    core_tokens: list = field(init=False)\n",
    "    true_labels: list = field(init=False)\n",
    "    pred_labels: list = field(init=False, default_factory=list)\n",
    "    sentence_ids: list = field(init=False)\n",
    "    token_positions: list = field(init=False)\n",
    "    token_selector_id: list = field(init=False)\n",
    "    agreements: list = field(init=False)\n",
    "    x: list = field(init=False)\n",
    "    y: list = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.last_hidden_states = torch.concat([s.last_hidden_states for s in self.model_outputs])\n",
    "        self.labels = torch.concat([s.labels for s in self.model_outputs])\n",
    "        self.losses = torch.concat([s.losses for s in self.model_outputs])\n",
    "        self.token_ids = torch.concat([s.input_ids for s in self.model_outputs])\n",
    "        self.words = [word for sentence in self.tokenization_outputs for word in sentence.words_df]\n",
    "        self.tokens = [token for sentence in self.tokenization_outputs for token in sentence.tokens_df]\n",
    "        self.word_pieces = [wp for sentence in self.tokenization_outputs for wp in sentence.word_pieces_df]\n",
    "        self.core_tokens = [ct for sentence in self.tokenization_outputs for ct in sentence.core_tokens_df]\n",
    "        self.true_labels = [label for sentence in self.tokenization_outputs for label in sentence.labels_df]\n",
    "        self.sentence_ids = [index for sentence in self.tokenization_outputs for index in sentence.sentence_index_df]\n",
    "        self.token_positions = [position for sentence in self.tokenization_outputs for position in range(len(sentence.tokens_df))]\n",
    "        self.token_selector_id = [\n",
    "            f\"{core_token}@#{token_position}@#{sentence_index}\" \n",
    "            for core_token, token_position, sentence_index in\n",
    "            zip(self.core_tokens, self.token_positions, self.sentence_ids)\n",
    "        ]\n",
    "        self.agreements = np.array(self.true_labels) == np.array(self.pred_labels)\n",
    "    \n",
    "    def align_labels(self, aligner):\n",
    "        aligned_labels = aligner.align_labels()\n",
    "        self.pred_labels = [label for sentence in aligned_labels for label in sentence]\n",
    "    \n",
    "    def assign_coordinate(self, coordinates):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        \n",
    "    def to_dict(self):\n",
    "        analysis_data = {\n",
    "            \"labels\": self.labels,\n",
    "            \"losses\": self.losses,\n",
    "            \"token_ids\": self.token_ids,\n",
    "            \"words\": self.words,\n",
    "            \"tokens\": self.tokens,\n",
    "            \"word_pieces\": self.word_pieces,\n",
    "            \"core_tokens\": self.core_tokens,\n",
    "            \"true_labels\": self.true_labels,\n",
    "            \"pred_labels\": self.pred_labels,\n",
    "            \"sentence_ids\": self.sentence_ids,\n",
    "            \"token_positions\": self.token_positions,\n",
    "            \"token_selector_id\": self.token_selector_id,\n",
    "            \"agreements\": self.agreements,\n",
    "            \"x\": self.x,\n",
    "            \"y\": self.y\n",
    "        }\n",
    "        return analysis_data\n",
    "        \n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class UMAPConfig:\n",
    "    n_neighbors: int = 15\n",
    "    min_dist: float = 0.1\n",
    "    metric: str = \"cosine\"\n",
    "    random_state: int = 1\n",
    "    verbose: bool = True\n",
    "    normalize_embeddings: bool = False\n",
    "\n",
    "    def set_params(self, \n",
    "                   n_neighbors: Optional[int] = None, \n",
    "                   min_dist: Optional[float] = None, \n",
    "                   metric: Optional[str] = None, \n",
    "                   normalize_embeddings: Optional[bool] = None):\n",
    "        \"\"\"Optionally update UMAP parameters.\"\"\"\n",
    "        if n_neighbors is not None:\n",
    "            self.n_neighbors = n_neighbors\n",
    "        if min_dist is not None:\n",
    "            self.min_dist = min_dist\n",
    "        if metric is not None:\n",
    "            self.metric = metric\n",
    "        if normalize_embeddings is not None:\n",
    "            self.normalize_embeddings = normalize_embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(config_dict):\n",
    "        \"\"\"Create UMAPConfig from a dictionary.\"\"\"\n",
    "        return UMAPConfig(**config_dict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate UMAP configuration to ensure valid settings.\"\"\"\n",
    "        if not isinstance(self.n_neighbors, int) or self.n_neighbors <= 0:\n",
    "            raise ValueError(\"n_neighbors must be a positive integer.\")\n",
    "        if not isinstance(self.min_dist, float) or self.min_dist < 0:\n",
    "            raise ValueError(\"min_dist must be a non-negative float.\")\n",
    "        if not isinstance(self.metric, str):\n",
    "            raise ValueError(\"metric must be a string.\")\n",
    "        if not isinstance(self.verbose, bool):\n",
    "            raise ValueError(\"verbose must be a boolean.\")\n",
    "        if not isinstance(self.normalize_embeddings, bool):\n",
    "            raise ValueError(\"normalize_embeddings must be a boolean.\")\n",
    "\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, tokenization_outputs, model_outputs, results, transformer):\n",
    "        \"\"\"\n",
    "        Initialize the FlatDataExtractor class.\n",
    "\n",
    "        Args:\n",
    "            preparation: The DataPreparation object.\n",
    "            outputs: The outputs object containing batch outputs.\n",
    "            results: The results object containing model results.\n",
    "        \"\"\"\n",
    "        self.aligner = LabelAligner(results.entity_outputs['y_pred'].copy(), tokenization_outputs)\n",
    "        self.tokenization_outputs = tokenization_outputs\n",
    "        self.model_outputs = model_outputs\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def extract_flat_data(self):\n",
    "        \"\"\"\n",
    "        Extract and flatten the data from the preparation, outputs, and results.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of flattened data elements.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.analysis_data = AnalysisData(\n",
    "            tokenization_outputs=self.tokenization_outputs,\n",
    "            model_outputs=self.model_outputs,\n",
    "        )\n",
    "        # analysis_data.align_labels(self.aligner)\n",
    "        # coordinates = self.transformer.apply_umap(analysis_data.last_hidden_states)\n",
    "        # analysis_data.assign_coordinate(coordinates)\n",
    "        # return analysis_data.to_dict()\n",
    "        \n",
    "        # flat_last_hidden_states = torch.concat(\n",
    "        #     [\n",
    "        #         sentence.last_hidden_states for sentence in self.model_outputs\n",
    "        #     ]\n",
    "        #     )\n",
    "\n",
    "        # flat_labels = torch.concat(\n",
    "        #     [\n",
    "        #         sentence.labels for sentence in self.model_outputs\n",
    "        #     ]\n",
    "        #     )\n",
    "\n",
    "\n",
    "        # flat_losses = torch.concat(\n",
    "        #     [\n",
    "        #         sentence.losses for sentence in self.model_outputs\n",
    "        #     ]\n",
    "        #     )\n",
    "\n",
    "        # flat_token_ids = torch.concat(\n",
    "        #     [\n",
    "        #         sentence.input_ids for sentence in self.model_outputs\n",
    "        #     ]\n",
    "        #     )\n",
    "\n",
    "\n",
    "        # flat_words = [\n",
    "        #     word for sentence in self.tokenization_outputs\n",
    "        #     for word in sentence.words_df\n",
    "        # ]\n",
    "\n",
    "        # flat_tokens = [\n",
    "        #     token for sentence in self.tokenization_outputs\n",
    "        #     for token in sentence.tokens_df\n",
    "        # ]\n",
    "\n",
    "        # flat_word_pieces = [\n",
    "        #     word_piece for sentence in self.tokenization_outputs\n",
    "        #     for word_piece in sentence.word_pieces_df\n",
    "        # ]\n",
    "\n",
    "        # flat_core_tokens = [\n",
    "        #     core_token for sentence in self.tokenization_outputs\n",
    "        #     for core_token in sentence.core_tokens_df\n",
    "        # ]\n",
    "\n",
    "        # flat_true_labels = [\n",
    "        #     label for sentence in self.tokenization_outputs\n",
    "        #     for label in sentence.labels_df\n",
    "        # ]\n",
    "\n",
    "        # flat_pred_labels = [\n",
    "        #     label for sentence in self.aligner.align_labels()\n",
    "        #     for label in sentence\n",
    "        # ]\n",
    "\n",
    "        # flat_sentence_ids = [\n",
    "        #     sentence_index for sentence in self.tokenization_outputs\n",
    "        #     for sentence_index in sentence.sentence_index_df\n",
    "        # ]\n",
    "\n",
    "        # flat_token_positions = [\n",
    "        #     position for sentence in self.tokenization_outputs\n",
    "        #     for position in range(len(sentence.tokens_df))\n",
    "        # ]\n",
    "\n",
    "        # flat_token_selector_id = [\n",
    "        #     f\"{core_token}@#{token_position}@#{sentence_index}\" \n",
    "        #     for core_token, token_position, sentence_index in\n",
    "        #     zip(flat_core_tokens, flat_token_positions, flat_sentence_ids)\n",
    "        # ]\n",
    "\n",
    "        # flat_agreements = np.array(flat_true_labels) == np.array(flat_pred_labels)\n",
    "        # analysis_data = {\n",
    "        #     \"flat_labels\": flat_labels,\n",
    "        #     \"flat_losses\": flat_losses,\n",
    "        #     \"flat_token_ids\": flat_token_ids,\n",
    "        #     \"flat_words\": flat_words,\n",
    "        #     \"flat_tokens\": flat_tokens,\n",
    "        #     \"flat_word_pieces\": flat_word_pieces,\n",
    "        #     \"flat_core_tokens\": flat_core_tokens,\n",
    "        #     \"flat_true_labels\": flat_true_labels,\n",
    "        #     \"flat_pred_labels\": flat_pred_labels,\n",
    "        #     \"flat_sentence_ids\": flat_sentence_ids,\n",
    "        #     \"flat_token_positions\": flat_token_positions,\n",
    "        #     \"flat_token_selector_id\": flat_token_selector_id,\n",
    "        #     \"flat_agreements\": flat_agreements\n",
    "        # }\n",
    "\n",
    "\n",
    "        # return flat_last_hidden_states, analysis_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_config = UMAPConfig.from_dict(config.get('umap'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, umap_config):\n",
    "        \"\"\"\n",
    "        Initialize the DataTransformer with configuration for UMAP.\n",
    "        \"\"\"\n",
    "        self.umap_config = umap_config\n",
    "\n",
    "    def apply_umap(self, data):\n",
    "        \"\"\"\n",
    "        Apply UMAP dimensionality reduction to the given data.\n",
    "        \"\"\"\n",
    "        if self.umap_config.normalize_embeddings:\n",
    "            data = normalize(data, axis=1)\n",
    "        umap_model = UMAP(\n",
    "            n_neighbors=self.umap_config.n_neighbors,\n",
    "            min_dist=self.umap_config.min_dist,\n",
    "            metric=self.umap_config.metric,\n",
    "            random_state=self.umap_config.random_state,\n",
    "            verbose=self.umap_config.verbose,\n",
    "        )\n",
    "        return umap_model.fit_transform(data).transpose()\n",
    "        # return umap_model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class DataTransformer:\n",
    "#     def __init__(self, umap_config, flat_states):\n",
    "#         \"\"\"\n",
    "#         Initialize the DataTransformer class with UMAP parameters and normalization option.\n",
    "\n",
    "#         Args:\n",
    "#             umap_config: configuration of umap paramaters\n",
    "#         \"\"\"\n",
    "#         self.flat_states = flat_states\n",
    "\n",
    "#         self.umap_model = UMAP(\n",
    "#             n_neighbors=umap_config.n_neighbors,\n",
    "#             min_dist=umap_config.min_dist,\n",
    "#             metric=umap_config.metric,\n",
    "#             random_state=umap_config.random_state,\n",
    "#             verbose=umap_config.verbose,\n",
    "#         )\n",
    "#         self.normalize_embeddings = umap_config.normalize_embeddings\n",
    "\n",
    "#     def apply_umap(self):\n",
    "#         \"\"\"\n",
    "#         Apply UMAP dimensionality reduction to the given flat states.\n",
    "\n",
    "#         Args:\n",
    "#             flat_states: The high-dimensional data to reduce.\n",
    "\n",
    "#         Returns:\n",
    "#             The reduced data.\n",
    "#         \"\"\"\n",
    "#         if self.normalize_embeddings:\n",
    "#             flat_states = normalize(self.flat_states, axis=1)\n",
    "#         return self.umap_model.fit_transform(flat_states).transpose()\n",
    "    \n",
    "    \n",
    "\n",
    "#     @staticmethod\n",
    "#     def transform_to_dataframe(self):\n",
    "#         coordinates = self.apply_umap()\n",
    "#         return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = DataTransformer(umap_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data_extractor = DataExtractor(tokenization_outputs.test, model_outputs.test, results, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/7h5mlrd57hg7h3l2cjpzwz7r000zbz/T/ipykernel_15468/1856640133.py:93: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  self.agreements = np.array(self.true_labels) == np.array(self.pred_labels)\n",
      "OMP: Info #273: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', random_state=1, verbose=True)\n",
      "Wed Jul 24 21:14:03 2024 Construct fuzzy simplicial set\n",
      "Wed Jul 24 21:14:03 2024 Finding Nearest Neighbors\n",
      "Wed Jul 24 21:14:03 2024 Building RP forest with 14 trees\n",
      "Wed Jul 24 21:14:08 2024 NN descent for 15 iterations\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "analysis_data_extractor.extract_flat_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = LabelAligner(results, tokenization_outputs.test['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.000000\n",
       "1        0.305853\n",
       "2        0.021982\n",
       "3        0.001037\n",
       "4        0.022747\n",
       "           ...   \n",
       "29706    0.000000\n",
       "29707    0.005826\n",
       "29708    0.003222\n",
       "29709    0.000037\n",
       "29710    0.000000\n",
       "Name: losses, Length: 29711, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df['losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_aligner.change_preds(results.entity_outputs['y_pred'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PERS',\n",
       " 'I-PERS',\n",
       " 'IGNORED',\n",
       " 'IGNORED',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PERS',\n",
       " 'I-PERS',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'IGNORED',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'IGNORED',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization_outputs.test['tokenized_text'][0].labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PERS',\n",
       " 'I-PERS',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PERS',\n",
       " 'I-PERS',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.entity_outputs['y_pred'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PERS',\n",
       " 'I-PERS',\n",
       " 'IGNORED',\n",
       " 'IGNORED',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PERS',\n",
       " 'I-PERS',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'IGNORED',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'IGNORED',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "analysis_df = pd.read_json(\n",
    "    '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Class Imbalance/3_errorAnalysis/Analysis/ANERCorp_CamelLab/test/test_analysis_df.jsonl.gz',\n",
    "    lines=True\n",
    "    )\n",
    "# seqeval calculations for entity confusion matrix\n",
    "confusion_data = pd.read_json(\n",
    "    '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Class Imbalance/3_errorAnalysis/Analysis/ANERCorp_CamelLab/test/test_confusion_data.jsonl.gz',\n",
    "    lines=True\n",
    "    )\n",
    "\n",
    "\n",
    "controid_df = pd.read_json(\n",
    "    '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Class Imbalance/3_errorAnalysis/Analysis/ANERCorp_CamelLab/test/test_controid_df.jsonl.gz',\n",
    "    lines=True\n",
    "    )\n",
    "\n",
    "\n",
    "entity_prediction = pd.read_json(\n",
    "    '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Class Imbalance/3_errorAnalysis/Analysis/ANERCorp_CamelLab/test/test_entity_prediction.jsonl.gz',\n",
    "    lines=True\n",
    "    )\n",
    "\n",
    "\n",
    "# this is just the x and y coordinates of the pretrained model\n",
    "pretrained_df = pd.read_json(\n",
    "    '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Class Imbalance/3_errorAnalysis/Analysis/ANERCorp_CamelLab/test/test_pre_df.jsonl.gz',\n",
    "    lines=True\n",
    "    )\n",
    "# this is exact same information but with extra truth and pred score\n",
    "token_score_df = pd.read_json(\n",
    "    '/Users/ay227/Library/CloudStorage/GoogleDrive-ahmed.younes.sam@gmail.com/My Drive/Final Year Experiments/Class Imbalance/3_errorAnalysis/Analysis/ANERCorp_CamelLab/test/test_token_score_df.jsonl.gz',\n",
    "    lines=True\n",
    "    )\n",
    "\n",
    "# Think about away of loading weights and activations\n",
    "# think about where the auxilary stuff should be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['global_id', 'token_id', 'word_id', 'sen_id', 'token_ids', 'label_ids',\n",
       "       'first_tokens_freq', 'first_tokens_consistency',\n",
       "       'first_tokens_inconsistency', 'words', 'wordpieces', 'tokens',\n",
       "       'first_tokens', 'truth', 'pred', 'agreement', 'losses', 'x', 'y',\n",
       "       'tokenization_rate', 'token_entropy', 'word_entropy', 'tr_entity',\n",
       "       'pr_entity', 'error_type', 'prediction_entropy', 'confidences',\n",
       "       'variability', 'O', 'B-PERS', 'I-PERS', 'B-ORG', 'I-ORG', 'B-LOC',\n",
       "       'I-LOC', 'B-MISC', 'I-MISC', '3_clusters', '4_clusters', '9_clusters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_ids</th>\n",
       "      <th>truth</th>\n",
       "      <th>pred</th>\n",
       "      <th>agreement</th>\n",
       "      <th>error_type</th>\n",
       "      <th>centroid</th>\n",
       "      <th>clusters</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@#0@#1</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>True</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Centroid-3</td>\n",
       "      <td>cluster-2</td>\n",
       "      <td>7.458792</td>\n",
       "      <td>-4.840983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@#0@#2</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>True</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Centroid-3</td>\n",
       "      <td>cluster-2</td>\n",
       "      <td>7.550399</td>\n",
       "      <td>-4.857412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-@#0@#3</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Centroid-3</td>\n",
       "      <td>cluster-1</td>\n",
       "      <td>1.931367</td>\n",
       "      <td>17.854967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@#0@#4</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Centroid-3</td>\n",
       "      <td>cluster-0</td>\n",
       "      <td>-4.481947</td>\n",
       "      <td>12.181932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@#0@#5</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>True</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Centroid-3</td>\n",
       "      <td>cluster-0</td>\n",
       "      <td>-6.245377</td>\n",
       "      <td>9.296581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74984</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>Centroid-9</td>\n",
       "      <td>C</td>\n",
       "      <td>10.065314</td>\n",
       "      <td>-6.042493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74985</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>Centroid-9</td>\n",
       "      <td>C</td>\n",
       "      <td>13.743145</td>\n",
       "      <td>-4.104878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74986</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>Centroid-9</td>\n",
       "      <td>C</td>\n",
       "      <td>0.448427</td>\n",
       "      <td>3.712907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74987</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>Centroid-9</td>\n",
       "      <td>C</td>\n",
       "      <td>1.370305</td>\n",
       "      <td>1.797442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74988</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>Centroid-9</td>\n",
       "      <td>C</td>\n",
       "      <td>8.327269</td>\n",
       "      <td>0.659489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74989 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_ids   truth    pred agreement error_type    centroid  \\\n",
       "0      @#0@#1   B-LOC   B-LOC      True    Correct  Centroid-3   \n",
       "1        @#0@#2   B-LOC   B-LOC      True    Correct  Centroid-3   \n",
       "2             -@#0@#3       O       O      True    Correct  Centroid-3   \n",
       "3           @#0@#4  B-PERS  B-PERS      True    Correct  Centroid-3   \n",
       "4          @#0@#5  I-PERS  I-PERS      True    Correct  Centroid-3   \n",
       "...               ...     ...     ...       ...        ...         ...   \n",
       "74984               C       C       C         C          C  Centroid-9   \n",
       "74985               C       C       C         C          C  Centroid-9   \n",
       "74986               C       C       C         C          C  Centroid-9   \n",
       "74987               C       C       C         C          C  Centroid-9   \n",
       "74988               C       C       C         C          C  Centroid-9   \n",
       "\n",
       "        clusters          x          y  \n",
       "0      cluster-2   7.458792  -4.840983  \n",
       "1      cluster-2   7.550399  -4.857412  \n",
       "2      cluster-1   1.931367  17.854967  \n",
       "3      cluster-0  -4.481947  12.181932  \n",
       "4      cluster-0  -6.245377   9.296581  \n",
       "...          ...        ...        ...  \n",
       "74984          C  10.065314  -6.042493  \n",
       "74985          C  13.743145  -4.104878  \n",
       "74986          C   0.448427   3.712907  \n",
       "74987          C   1.370305   1.797442  \n",
       "74988          C   8.327269   0.659489  \n",
       "\n",
       "[74989 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50          O\n",
       "51    IGNORED\n",
       "52          O\n",
       "53          O\n",
       "54          O\n",
       "55          O\n",
       "56          O\n",
       "57          O\n",
       "58          O\n",
       "59          O\n",
       "60          O\n",
       "61          O\n",
       "62          O\n",
       "63          O\n",
       "64          O\n",
       "65          O\n",
       "66          O\n",
       "67          O\n",
       "68          O\n",
       "69          O\n",
       "70          O\n",
       "71          O\n",
       "72      B-LOC\n",
       "73      B-LOC\n",
       "74    IGNORED\n",
       "75          O\n",
       "76          O\n",
       "77          O\n",
       "78          O\n",
       "79    IGNORED\n",
       "80    IGNORED\n",
       "81          O\n",
       "82          O\n",
       "83          O\n",
       "84      [SEP]\n",
       "85      [CLS]\n",
       "86          O\n",
       "87          O\n",
       "88          O\n",
       "89          O\n",
       "90          O\n",
       "91          O\n",
       "92          O\n",
       "93          O\n",
       "94          O\n",
       "95          O\n",
       "96    IGNORED\n",
       "97          O\n",
       "98          O\n",
       "99          O\n",
       "Name: pred, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df['pred'].iloc[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29711"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(analysis_df['truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tr_entity</th>\n",
       "      <th>pr_entity</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29706</th>\n",
       "      <td>IGNORED</td>\n",
       "      <td>IGNORED</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29707</th>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29708</th>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29709</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29710</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29711 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tr_entity pr_entity error_type\n",
       "0         [CLS]     [CLS]    Correct\n",
       "1           LOC       LOC    Correct\n",
       "2           LOC       LOC    Correct\n",
       "3             O         O    Correct\n",
       "4          PERS      PERS    Correct\n",
       "...         ...       ...        ...\n",
       "29706   IGNORED   IGNORED    Correct\n",
       "29707      PERS      PERS    Correct\n",
       "29708      PERS      PERS    Correct\n",
       "29709         O         O    Correct\n",
       "29710     [SEP]     [SEP]    Correct\n",
       "\n",
       "[29711 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df[['tr_entity',\n",
    "       'pr_entity', 'error_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PERS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PERS</td>\n",
       "      <td>PERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LOC</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   truth  pred\n",
       "0    LOC   LOC\n",
       "1    LOC   LOC\n",
       "2   PERS  PERS\n",
       "3   PERS  PERS\n",
       "4    LOC   LOC\n",
       "5    LOC   LOC\n",
       "6   PERS     O\n",
       "7    LOC   LOC\n",
       "8    LOC   LOC\n",
       "9      O   LOC\n",
       "10     O   LOC\n",
       "11   LOC   LOC\n",
       "12   LOC   LOC\n",
       "13  PERS  PERS\n",
       "14   LOC   LOC\n",
       "15   LOC   LOC\n",
       "16  PERS  PERS\n",
       "17   LOC   LOC\n",
       "18   ORG   ORG\n",
       "19   ORG   ORG"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
